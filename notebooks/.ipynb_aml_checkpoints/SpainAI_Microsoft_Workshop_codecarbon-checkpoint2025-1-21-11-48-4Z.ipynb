{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2OJtDt541b3"
      },
      "source": [
        "# Workshop Microsoft - Spain IA - Sostenibilidad\n",
        "En este caso de uso usaremos un LLM pequeño para evaluar su impacto energético"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting transformers==4.48\n",
            "  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: accelerate in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (1.4.0)\n",
            "Requirement already satisfied: sentencepiece in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.2.0)\n",
            "Requirement already satisfied: tokenizer in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (3.4.5)\n",
            "Requirement already satisfied: codecarbon in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.8.3)\n",
            "Requirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (0.29.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (2024.11.6)\n",
            "Requirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (4.66.5)\n",
            "Requirement already satisfied: psutil in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate) (6.0.0)\n",
            "Requirement already satisfied: torch>=2.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate) (2.6.0)\n",
            "Requirement already satisfied: arrow in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (1.3.0)\n",
            "Requirement already satisfied: click in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (8.1.7)\n",
            "Requirement already satisfied: fief-client[cli] in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (1.3.5)\n",
            "Requirement already satisfied: prometheus-client in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (0.21.1)\n",
            "Requirement already satisfied: py-cpuinfo in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pynvml in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (12.0.0)\n",
            "Requirement already satisfied: questionary in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (2.1.0)\n",
            "Requirement already satisfied: rapidfuzz in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (3.12.1)\n",
            "Requirement already satisfied: rich in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (0.15.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48) (2023.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48) (4.12.2)\n",
            "Requirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0.20241206)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
            "Requirement already satisfied: yaspin in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (3.1.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->codecarbon) (2024.2)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pynvml->codecarbon) (12.570.86)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from questionary->codecarbon) (3.0.47)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.48) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.48) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.48) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.48) (2024.8.30)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.8.0)\n",
            "Requirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: termcolor<2.4.0,>=2.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from yaspin->fief-client[cli]->codecarbon) (2.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.2)\n",
            "Requirement already satisfied: pycparser in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n",
            "Downloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.49.0\n",
            "    Uninstalling transformers-4.49.0:\n",
            "      Successfully uninstalled transformers-4.49.0\n",
            "Successfully installed transformers-4.48.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers==4.48 accelerate sentencepiece tokenizer codecarbon\n",
        "%pip install dash dash-bootstrap-components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Reiniciar el kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1739966605365
        },
        "id": "0LyAUyQ4s3_7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "import textwrap\n",
        "from codecarbon import EmissionsTracker\n",
        "from codecarbon import track_emissions\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zv2Oll0647jO"
      },
      "source": [
        "## Modelo\n",
        "Usaremos mocrosoft Phi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
        "output_dir_emissions_track=\"/home/azureuser/cloudfiles/code/Users/paul.vanb\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "0e980c20fece4573a9a522f1e5d84e00",
            "fcef54490bae4d83b6ef6d36bbf46d84",
            "1ed491b9ba624120bc6b3e6db5b98cf6",
            "102b824d816e43259751e828e46a2566",
            "51921656a92c437e82dc96a0d14246e3",
            "b715b9944fa547a4b65e554dc5d9fd1e",
            "11dec34858fe493ba356c42e98dec0c7",
            "7240007e78444219878b593e7b49a93a",
            "bd317caff4c8479fa02a4403361e2463",
            "2523f8c0d6274cdd977884b215f516a3",
            "fb068428db9148efbf737dbff9e01867"
          ]
        },
        "gather": {
          "logged": 1739965639405
        },
        "id": "4RQLYiQHtdJV",
        "outputId": "064a0a14-77ff-4b88-ab6a-45dd23c2da67"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.11it/s]\n"
          ]
        }
      ],
      "source": [
        "torch.random.manual_seed(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    #device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWbfhHfD5Apa"
      },
      "source": [
        "Prueba con el texto de ejemplo de huggingface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1739965715630
        },
        "id": "-_A4S7myy4JL",
        "outputId": "dd57c30c-143e-43ce-c411-ba2151d5e273"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cpu\n",
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " To solve the linear equation 2x + 3 = 7, follow these steps:\n",
            "\n",
            "1. Subtract 3 from both sides of the equation to isolate the term with the variable (x):\n",
            "\n",
            "   2x + 3 - 3 = 7 - 3\n",
            "   2x = 4\n",
            "\n",
            "2. Now, divide both sides of the equation by the coefficient of x (which is 2):\n",
            "\n",
            "   2x / 2 = 4 / 2\n",
            "   x = 2\n",
            "\n",
            "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
        "]\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlQRIo-N3B_J"
      },
      "source": [
        "creamos funciones para organizar output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1739965768910
        },
        "id": "5r8b3DKDtpai"
      },
      "outputs": [],
      "source": [
        "\n",
        "def wrap_text(text, width=100):\n",
        "    lines = text.split('\\n')\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "    return wrapped_text\n",
        "\n",
        "def generate(input_text, system_prompt=\"\", max_length=1024):\n",
        "    if system_prompt != \"\":\n",
        "        system_prompt = system_prompt\n",
        "    else:\n",
        "        system_prompt = \"You are a friendly and helpful assistant\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\":  input_text},\n",
        "    ]\n",
        "\n",
        "    output = pipe(messages, **generation_args)\n",
        "    text = output[0]['generated_text']\n",
        "\n",
        "    wrapped_text = wrap_text(text)\n",
        "    return wrapped_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "gather": {
          "logged": 1739965804267
        },
        "id": "lnP0vvHsybz4",
        "outputId": "3eea63eb-8a9f-45d0-ced3-57f7fe918b28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Soy un modelo de lenguaje de IA desarrollado por Microsoft. No soy Marco Aurélio, un renombrado\n",
            "abogado brasileño, sino una inteligencia artificial diseñada para ayudar con una amplia gama de\n",
            "preguntas y tareas proporcionando información y asistencia.\n"
          ]
        }
      ],
      "source": [
        "print(generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres marcoaurelio, hijo de thor\",\n",
        "         ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FH_iNESv5LjH"
      },
      "source": [
        "# Consumo energético con CodeCarbon"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTK5TMmvtryP",
        "outputId": "3e488b30-0b67-4af7-c84f-7505c4ef1b29"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon INFO @ 15:49:22] Codecarbon is taking the configuration from global file: /home/azureuser/.codecarbon.config\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon INFO @ 15:49:23] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 15:49:23] [setup] CPU Tracking...\n",
            "[codecarbon DEBUG @ 15:49:23] Not using PowerGadget, an exception occurred while instantiating IntelPowerGadget : Platform not supported by Intel Power Gadget\n",
            "[codecarbon DEBUG @ 15:49:23] Not using the RAPL interface, an exception occurred while instantiating IntelRAPL : Intel RAPL files not found at /sys/class/powercap/intel-rapl on linux\n",
            "[codecarbon DEBUG @ 15:49:23] Not using PowerMetrics, an exception occurred while instantiating Powermetrics : Platform not supported by Powermetrics\n",
            "[codecarbon WARNING @ 15:49:23] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon DEBUG @ 15:49:24] CPU : We detect a Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz with a TDP of 195.0 W\n",
            "[codecarbon INFO @ 15:49:24] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n",
            "[codecarbon INFO @ 15:49:24] [setup] GPU Tracking...\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[codecarbon INFO @ 15:49:24] No GPU found.\n",
            "[codecarbon DEBUG @ 15:49:24] The below tracking methods have been set up:\n",
            "                RAM Tracking Method: 3 Watts for 8 GB ratio constant\n",
            "                CPU Tracking Method: TDP constant\n",
            "                GPU Tracking Method: Unspecified\n",
            "            \n",
            "[codecarbon INFO @ 15:49:24] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 15:49:24]   Platform system: Linux-5.15.0-1073-azure-x86_64-with-glibc2.31\n",
            "[codecarbon INFO @ 15:49:24]   Python version: 3.10.14\n",
            "[codecarbon INFO @ 15:49:24]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 15:49:24]   Available RAM : 31.343 GB\n",
            "[codecarbon INFO @ 15:49:24]   CPU count: 4\n",
            "[codecarbon INFO @ 15:49:24]   CPU model: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n",
            "[codecarbon INFO @ 15:49:24]   GPU count: None\n",
            "[codecarbon INFO @ 15:49:24]   GPU model: None\n",
            "[codecarbon DEBUG @ 15:49:24] Not running on AWS\n",
            "[codecarbon WARNING @ 15:49:24] Cloud provider 'azure' do not publish electricity carbon intensity. Using country value instead.\n",
            "[codecarbon INFO @ 15:49:24] Saving emissions data to file /home/azureuser/cloudfiles/code/Users/paul.vanb/emissions.csv\n",
            "[codecarbon INFO @ 15:49:39] Energy consumed for RAM : 0.000013 kWh. RAM Power : 3.0469021797180176 W\n",
            "[codecarbon DEBUG @ 15:49:39] RAM : 3.05 W during 15.00 s [measurement time: 0.0078]\n",
            "[codecarbon INFO @ 15:49:39] Energy consumed for all CPUs : 0.000407 kWh. Total CPU Power : 97.5 W\n",
            "[codecarbon DEBUG @ 15:49:39] CPU : 97.50 W during 15.02 s [measurement time: 0.0011]\n",
            "[codecarbon INFO @ 15:49:39] 0.000419 kWh of electricity used since the beginning.\n",
            "[codecarbon DEBUG @ 15:49:39] last_duration=15.001011504999951\n",
            "------------------------\n",
            "[codecarbon DEBUG @ 15:49:45] Removing the lock\n",
            "[codecarbon INFO @ 15:49:45] Energy consumed for RAM : 0.000017 kWh. RAM Power : 3.0470695495605473 W\n",
            "[codecarbon DEBUG @ 15:49:45] RAM : 3.05 W during 5.52 s [measurement time: 0.0052]\n",
            "[codecarbon INFO @ 15:49:45] Energy consumed for all CPUs : 0.000556 kWh. Total CPU Power : 97.5 W\n",
            "[codecarbon DEBUG @ 15:49:45] CPU : 97.50 W during 5.53 s [measurement time: 0.0006]\n",
            "[codecarbon INFO @ 15:49:45] 0.000574 kWh of electricity used since the beginning.\n",
            "[codecarbon DEBUG @ 15:49:45] last_duration=5.517389260000073\n",
            "------------------------\n",
            "[codecarbon DEBUG @ 15:49:45] EmissionsData(timestamp='2025-02-19T15:49:45', project_name='codecarbon', run_id='c29eaeff-7e73-4de2-81f5-2d7d7e121a0f', experiment_id='D08fa668-aed9-4ee9-a535-dedb1361039c', duration=20.548237257999972, emissions=4.862417284179631e-05, emissions_rate=2.366342778277277e-06, cpu_power=97.5, gpu_power=0.0, ram_power=3.0470695495605473, cpu_energy=0.0005563634338437482, gpu_energy=0, ram_energy=1.736626267157495e-05, energy_consumed=0.0005737296965153231, country_name='United States', country_iso_code='USA', region='washington', cloud_provider='', cloud_region='', os='Linux-5.15.0-1073-azure-x86_64-with-glibc2.31', python_version='3.10.14', codecarbon_version='2.8.3', cpu_count=4, cpu_model='Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz', gpu_count=None, gpu_model=None, longitude=-122.3303, latitude=47.6109, ram_total_size=31.343276977539062, tracking_mode='process', on_cloud='N', pue=1.0)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 41.1 s, sys: 92.1 ms, total: 41.2 s\n",
            "Wall time: 23.8 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "# Start tracking\n",
        "tracker = EmissionsTracker(tracking_mode=\"process\",\n",
        "                           output_dir=output_dir_emissions_track) # save_to_api=True,\n",
        "tracker.start()\n",
        "\n",
        "generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres un asistente majo\",\n",
        "         \n",
        "         )\n",
        "emmissions = tracker.stop()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All hardware identified: [RAM(), CPU(Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz > 195.0W)]\n",
            "GPU/CPU Power: Power(kW=0.0975)\n",
            "Hardware zone: GeoMetadata(country_iso_code=USA, country_name=United States, region=washington)\n",
            "Emissions from this training run: 0.000049 kg CO2eq\n"
          ]
        }
      ],
      "source": [
        "# Note that CodeCarbon will output the energy consumption every time you execute code\n",
        "# Comment out the `log_level` argument when instantiating the tracker to suppress this output\n",
        "print(\"All hardware identified:\", tracker._hardware)\n",
        "print(\"GPU/CPU Power:\", tracker._hardware[1].total_power())\n",
        "print(\"Hardware zone:\", tracker._geo)\n",
        "print(f\"Emissions from this training run: {emmissions:5f} kg CO2eq\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TH9Dp1VU5OoA"
      },
      "source": [
        "Usando with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1739966591814
        },
        "id": "3bVginYm4hvK",
        "outputId": "1719c56a-540f-4498-b166-7df01b4a443f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon INFO @ 15:55:20] Codecarbon is taking the configuration from global file: /home/azureuser/.codecarbon.config\n",
            "[codecarbon DEBUG @ 15:55:20] Lock file /tmp/.codecarbon.lock already exists. This usually means another instance of codecarbon is running. You can safely delete it if you want or use allow_multiple_runs parameter to always bypass it.\n",
            "[codecarbon ERROR @ 15:55:20] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
            "[codecarbon WARNING @ 15:55:20] Another instance of codecarbon is already running. Exiting.\n",
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "[codecarbon WARNING @ 15:55:40] Another instance of codecarbon is already running. Exiting.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "with EmissionsTracker(output_dir=output_dir_emissions_track) as tracker:\n",
        "  generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres un asistente majo\",\n",
        "         )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LYZbUxTh5RPF"
      },
      "source": [
        "Como decorador"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1739966628292
        },
        "id": "KfLriy2N1XrQ"
      },
      "outputs": [],
      "source": [
        "@track_emissions(project_name=\"workshop-sostenibilidad\")\n",
        "def answer_to_different_messages(messages):\n",
        "  outputs = []\n",
        "  system_prompt = \"You are a friendly and helpful assistant\"\n",
        "  for message in messages:\n",
        "    output = generate(message)\n",
        "    outputs.append(message)\n",
        "  return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "gather": {
          "logged": 1739966695605
        },
        "id": "sSlx0bA13eda",
        "outputId": "a62ba4e2-9162-4a79-d1de-e12d86328bfc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon INFO @ 16:02:33] Codecarbon is taking the configuration from global file: /home/azureuser/.codecarbon.config\n",
            "[codecarbon DEBUG @ 16:02:33] Lock file /tmp/.codecarbon.lock already exists. This usually means another instance of codecarbon is running. You can safely delete it if you want or use allow_multiple_runs parameter to always bypass it.\n",
            "[codecarbon ERROR @ 16:02:33] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
            "[codecarbon WARNING @ 16:02:33] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon INFO @ 16:03:52] \n",
            "Graceful stopping: collecting and writing information.\n",
            "Please wait a few seconds...\n",
            "[codecarbon WARNING @ 16:03:52] Another instance of codecarbon is already running. Exiting.\n",
            "[codecarbon INFO @ 16:03:52] Done!\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['hola!', 'cómo puedo hacerlo?', ' gracias!']"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "answer_to_different_messages([\"hola!\",\"cómo puedo hacerlo?\",\" gracias!\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "https://mlco2.github.io/codecarbon/output.html#output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1739966878051
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logs\n",
            "Users\n"
          ]
        }
      ],
      "source": [
        "%%sh\n",
        "ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1739966892343
        }
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>project_name</th>\n",
              "      <th>run_id</th>\n",
              "      <th>experiment_id</th>\n",
              "      <th>duration</th>\n",
              "      <th>emissions</th>\n",
              "      <th>emissions_rate</th>\n",
              "      <th>cpu_power</th>\n",
              "      <th>gpu_power</th>\n",
              "      <th>ram_power</th>\n",
              "      <th>...</th>\n",
              "      <th>cpu_count</th>\n",
              "      <th>cpu_model</th>\n",
              "      <th>gpu_count</th>\n",
              "      <th>gpu_model</th>\n",
              "      <th>longitude</th>\n",
              "      <th>latitude</th>\n",
              "      <th>ram_total_size</th>\n",
              "      <th>tracking_mode</th>\n",
              "      <th>on_cloud</th>\n",
              "      <th>pue</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2025-02-19T12:02:50</td>\n",
              "      <td>codecarbon</td>\n",
              "      <td>3bfd136c-d8e5-4f4e-a98e-a51239239023</td>\n",
              "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
              "      <td>17.135990</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>97.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.091317</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-122.3303</td>\n",
              "      <td>47.6109</td>\n",
              "      <td>31.343273</td>\n",
              "      <td>process</td>\n",
              "      <td>N</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2025-02-19T12:03:10</td>\n",
              "      <td>codecarbon</td>\n",
              "      <td>9bc68c95-44f7-4838-9b5e-00de148b7f57</td>\n",
              "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
              "      <td>17.254767</td>\n",
              "      <td>0.000044</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>97.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.753727</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-122.3303</td>\n",
              "      <td>47.6109</td>\n",
              "      <td>31.343273</td>\n",
              "      <td>machine</td>\n",
              "      <td>N</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2025-02-19T12:04:54</td>\n",
              "      <td>workshop-sostenibilidad</td>\n",
              "      <td>ef6ae2a3-bd22-469f-aa2e-e12b329f5886</td>\n",
              "      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n",
              "      <td>63.112533</td>\n",
              "      <td>0.000162</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>97.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11.753727</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-122.3303</td>\n",
              "      <td>47.6109</td>\n",
              "      <td>31.343273</td>\n",
              "      <td>machine</td>\n",
              "      <td>N</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2025-02-19T15:49:45</td>\n",
              "      <td>codecarbon</td>\n",
              "      <td>c29eaeff-7e73-4de2-81f5-2d7d7e121a0f</td>\n",
              "      <td>D08fa668-aed9-4ee9-a535-dedb1361039c</td>\n",
              "      <td>20.548237</td>\n",
              "      <td>0.000049</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>97.5</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.047070</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>-122.3303</td>\n",
              "      <td>47.6109</td>\n",
              "      <td>31.343277</td>\n",
              "      <td>process</td>\n",
              "      <td>N</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             timestamp             project_name  \\\n",
              "0  2025-02-19T12:02:50               codecarbon   \n",
              "1  2025-02-19T12:03:10               codecarbon   \n",
              "2  2025-02-19T12:04:54  workshop-sostenibilidad   \n",
              "3  2025-02-19T15:49:45               codecarbon   \n",
              "\n",
              "                                 run_id                         experiment_id  \\\n",
              "0  3bfd136c-d8e5-4f4e-a98e-a51239239023  5b0fa12a-3dd7-45bb-9766-cc326314d9f1   \n",
              "1  9bc68c95-44f7-4838-9b5e-00de148b7f57  5b0fa12a-3dd7-45bb-9766-cc326314d9f1   \n",
              "2  ef6ae2a3-bd22-469f-aa2e-e12b329f5886  5b0fa12a-3dd7-45bb-9766-cc326314d9f1   \n",
              "3  c29eaeff-7e73-4de2-81f5-2d7d7e121a0f  D08fa668-aed9-4ee9-a535-dedb1361039c   \n",
              "\n",
              "    duration  emissions  emissions_rate  cpu_power  gpu_power  ram_power  ...  \\\n",
              "0  17.135990   0.000041        0.000002       97.5        0.0   3.091317  ...   \n",
              "1  17.254767   0.000044        0.000003       97.5        0.0  11.753727  ...   \n",
              "2  63.112533   0.000162        0.000003       97.5        0.0  11.753727  ...   \n",
              "3  20.548237   0.000049        0.000002       97.5        0.0   3.047070  ...   \n",
              "\n",
              "   cpu_count                                       cpu_model  gpu_count  \\\n",
              "0          4  Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz        NaN   \n",
              "1          4  Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz        NaN   \n",
              "2          4  Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz        NaN   \n",
              "3          4  Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz        NaN   \n",
              "\n",
              "   gpu_model longitude latitude ram_total_size  tracking_mode  on_cloud  pue  \n",
              "0        NaN -122.3303  47.6109      31.343273        process         N  1.0  \n",
              "1        NaN -122.3303  47.6109      31.343273        machine         N  1.0  \n",
              "2        NaN -122.3303  47.6109      31.343273        machine         N  1.0  \n",
              "3        NaN -122.3303  47.6109      31.343277        process         N  1.0  \n",
              "\n",
              "[4 rows x 32 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_emissions = pd.read_csv(os.path.join(output_dir_emissions_track,\"emissions.csv\"))\n",
        "df_emissions.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU5UVLwy4yGh"
      },
      "source": [
        "# Uso de la API\n",
        "\n",
        "https://dashboard.codecarbon.io/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvR0gxrO9Dta",
        "outputId": "c766c678-0056-490e-ed4e-b7bb00641f64"
      },
      "outputs": [],
      "source": [
        "# en local\n",
        "#!codecarbon login\n",
        "#!codecarbon config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xG0X4Rg8hym",
        "outputId": "aa154879-5fdf-4db8-e11d-8c8b96c90859"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[codecarbon INFO @ 16:05:21] Codecarbon is taking the configuration from global file: /home/azureuser/.codecarbon.config\n",
            "[codecarbon DEBUG @ 16:05:21] Lock file /tmp/.codecarbon.lock already exists. This usually means another instance of codecarbon is running. You can safely delete it if you want or use allow_multiple_runs parameter to always bypass it.\n",
            "[codecarbon ERROR @ 16:05:21] Error: Another instance of codecarbon is probably running as we find `/tmp/.codecarbon.lock`. Turn off the other instance to be able to run this one or use `allow_multiple_runs` or delete the file. Exiting.\n",
            "[codecarbon WARNING @ 16:05:21] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "[codecarbon WARNING @ 16:05:42] Another instance of codecarbon is already running. Exiting.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 40.8 s, sys: 20 ms, total: 40.8 s\n",
            "Wall time: 20.6 s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "%%time\n",
        "# Start tracking\n",
        "tracker = EmissionsTracker(tracking_mode=\"process\", save_to_api=True, )\n",
        "tracker.start()\n",
        "\n",
        "generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres un asistente majo\",\n",
        "         )\n",
        "emmissions = tracker.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROD1radl8jfp"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e980c20fece4573a9a522f1e5d84e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcef54490bae4d83b6ef6d36bbf46d84",
              "IPY_MODEL_1ed491b9ba624120bc6b3e6db5b98cf6",
              "IPY_MODEL_102b824d816e43259751e828e46a2566"
            ],
            "layout": "IPY_MODEL_51921656a92c437e82dc96a0d14246e3"
          }
        },
        "102b824d816e43259751e828e46a2566": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2523f8c0d6274cdd977884b215f516a3",
            "placeholder": "​",
            "style": "IPY_MODEL_fb068428db9148efbf737dbff9e01867",
            "value": " 2/2 [00:37&lt;00:00, 17.82s/it]"
          }
        },
        "11dec34858fe493ba356c42e98dec0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ed491b9ba624120bc6b3e6db5b98cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7240007e78444219878b593e7b49a93a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd317caff4c8479fa02a4403361e2463",
            "value": 2
          }
        },
        "2523f8c0d6274cdd977884b215f516a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51921656a92c437e82dc96a0d14246e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7240007e78444219878b593e7b49a93a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b715b9944fa547a4b65e554dc5d9fd1e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd317caff4c8479fa02a4403361e2463": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fb068428db9148efbf737dbff9e01867": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fcef54490bae4d83b6ef6d36bbf46d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b715b9944fa547a4b65e554dc5d9fd1e",
            "placeholder": "​",
            "style": "IPY_MODEL_11dec34858fe493ba356c42e98dec0c7",
            "value": "Loading checkpoint shards: 100%"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
