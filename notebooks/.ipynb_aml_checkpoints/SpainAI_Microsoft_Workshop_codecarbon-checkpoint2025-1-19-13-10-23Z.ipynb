{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop Microsoft - Spain IA - Sostenibilidad\n",
        "En este caso de uso usaremos un LLM pequeño para evaluar su impacto energético"
      ],
      "metadata": {
        "id": "-2OJtDt541b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install transformers==4.48 accelerate sentencepiece tokenizer codecarbon"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Collecting transformers==4.48\n  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\nRequirement already satisfied: accelerate in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (1.4.0)\nRequirement already satisfied: sentencepiece in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (0.2.0)\nRequirement already satisfied: tokenizer in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (3.4.5)\nRequirement already satisfied: codecarbon in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (2.8.3)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (3.16.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (24.1)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (2024.11.6)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (0.5.2)\nRequirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.48) (4.66.5)\nRequirement already satisfied: psutil in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate) (6.0.0)\nRequirement already satisfied: torch>=2.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from accelerate) (2.6.0)\nRequirement already satisfied: arrow in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (1.3.0)\nRequirement already satisfied: click in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (8.1.7)\nRequirement already satisfied: fief-client[cli] in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (0.20.0)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (1.3.5)\nRequirement already satisfied: prometheus-client in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (0.21.1)\nRequirement already satisfied: py-cpuinfo in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (9.0.0)\nRequirement already satisfied: pynvml in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (12.0.0)\nRequirement already satisfied: questionary in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (2.1.0)\nRequirement already satisfied: rapidfuzz in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (3.12.1)\nRequirement already satisfied: rich in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (13.9.4)\nRequirement already satisfied: typer in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from codecarbon) (0.15.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48) (2023.10.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48) (4.12.2)\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.3)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.4)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.5.8)\nRequirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.2.1.3)\nRequirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.5.147)\nRequirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.6.1.9)\nRequirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.3.1.170)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.4.127)\nRequirement already satisfied: triton==3.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.7.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0)\nRequirement already satisfied: types-python-dateutil>=2.8.10 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from arrow->codecarbon) (2.9.0.20241206)\nRequirement already satisfied: httpx<0.28.0,>=0.21.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (0.27.2)\nRequirement already satisfied: jwcrypto<2.0.0,>=1.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (1.5.6)\nRequirement already satisfied: yaspin in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fief-client[cli]->codecarbon) (3.1.0)\nRequirement already satisfied: pytz>=2017.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas->codecarbon) (2024.2)\nRequirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pynvml->codecarbon) (12.570.86)\nRequirement already satisfied: prompt_toolkit<4.0,>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from questionary->codecarbon) (3.0.47)\nRequirement already satisfied: charset-normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.48) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.48) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.48) (2.2.3)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.48) (2024.8.30)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rich->codecarbon) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from rich->codecarbon) (2.18.0)\nRequirement already satisfied: shellingham>=1.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from typer->codecarbon) (1.5.4)\nRequirement already satisfied: anyio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (4.8.0)\nRequirement already satisfied: httpcore==1.* in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\nRequirement already satisfied: sniffio in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\nRequirement already satisfied: h11<0.15,>=0.13 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\nRequirement already satisfied: cryptography>=3.4 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.1)\nRequirement already satisfied: mdurl~=0.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\nRequirement already satisfied: wcwidth in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\nRequirement already satisfied: termcolor<2.4.0,>=2.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from yaspin->fief-client[cli]->codecarbon) (2.3.0)\nRequirement already satisfied: cffi>=1.12 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\nRequirement already satisfied: exceptiongroup>=1.0.2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from anyio->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.2.2)\nRequirement already satisfied: pycparser in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\nDownloading transformers-4.48.0-py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m98.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.49.0\n    Uninstalling transformers-4.49.0:\n      Successfully uninstalled transformers-4.49.0\nSuccessfully installed transformers-4.48.0\nNote: you may need to restart the kernel to use updated packages.\n"
        }
      ],
      "execution_count": 9,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reiniciar el kernel"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "import textwrap\n",
        "from codecarbon import EmissionsTracker\n",
        "from codecarbon import track_emissions\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "import torch"
      ],
      "outputs": [],
      "execution_count": 10,
      "metadata": {
        "id": "0LyAUyQ4s3_7",
        "gather": {
          "logged": 1739966605365
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo\n",
        "Usaremos mocrosoft Phi"
      ],
      "metadata": {
        "id": "Zv2Oll0647jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "    #device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\nCurrent `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n\rLoading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]\rLoading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  5.03it/s]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.32it/s]\rLoading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  5.26it/s]\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "0e980c20fece4573a9a522f1e5d84e00",
            "fcef54490bae4d83b6ef6d36bbf46d84",
            "1ed491b9ba624120bc6b3e6db5b98cf6",
            "102b824d816e43259751e828e46a2566",
            "51921656a92c437e82dc96a0d14246e3",
            "b715b9944fa547a4b65e554dc5d9fd1e",
            "11dec34858fe493ba356c42e98dec0c7",
            "7240007e78444219878b593e7b49a93a",
            "bd317caff4c8479fa02a4403361e2463",
            "2523f8c0d6274cdd977884b215f516a3",
            "fb068428db9148efbf737dbff9e01867"
          ]
        },
        "id": "4RQLYiQHtdJV",
        "outputId": "064a0a14-77ff-4b88-ab6a-45dd23c2da67",
        "gather": {
          "logged": 1739965639405
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba con el texto de ejemplo de huggingface"
      ],
      "metadata": {
        "id": "AWbfhHfD5Apa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
        "]\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Device set to use cpu\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\nThe `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\nYou are not running the flash-attention implementation, expect numerical differences.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": " To solve the linear equation 2x + 3 = 7, follow these steps:\n\n1. Subtract 3 from both sides of the equation to isolate the term with the variable (x):\n\n   2x + 3 - 3 = 7 - 3\n   2x = 4\n\n2. Now, divide both sides of the equation by the coefficient of x (which is 2):\n\n   2x / 2 = 4 / 2\n   x = 2\n\nSo, the solution to the equation 2x + 3 = 7 is x = 2.\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_A4S7myy4JL",
        "outputId": "dd57c30c-143e-43ce-c411-ba2151d5e273",
        "gather": {
          "logged": 1739965715630
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "creamos funciones para organizar output"
      ],
      "metadata": {
        "id": "PlQRIo-N3B_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def wrap_text(text, width=100):\n",
        "    lines = text.split('\\n')\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "    return wrapped_text\n",
        "\n",
        "def generate(input_text, system_prompt=\"\", max_length=1024):\n",
        "    if system_prompt != \"\":\n",
        "        system_prompt = system_prompt\n",
        "    else:\n",
        "        system_prompt = \"You are a friendly and helpful assistant\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\":  input_text},\n",
        "    ]\n",
        "\n",
        "    output = pipe(messages, **generation_args)\n",
        "    text = output[0]['generated_text']\n",
        "\n",
        "    wrapped_text = wrap_text(text)\n",
        "    return wrapped_text"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "id": "5r8b3DKDtpai",
        "gather": {
          "logged": 1739965768910
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres marcoaurelio, hijo de thor\",\n",
        "         ))"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 5,
          "data": {
            "text/plain": "' Soy un modelo de lenguaje de IA desarrollado por Microsoft. No soy Marco Aurélio, un renombrado\\nabogado brasileño, sino una inteligencia artificial diseñada para ayudar con una amplia gama de\\npreguntas y tareas proporcionando información y asistencia.'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "lnP0vvHsybz4",
        "outputId": "3eea63eb-8a9f-45d0-ced3-57f7fe918b28",
        "gather": {
          "logged": 1739965804267
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Consumo energético con CodeCarbon"
      ],
      "metadata": {
        "id": "FH_iNESv5LjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Start tracking\n",
        "tracker = EmissionsTracker(tracking_mode=\"process\") # save_to_api=True,\n",
        "tracker.start()\n",
        "\n",
        "generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres un asistente majo\",\n",
        "         )\n",
        "emmissions = tracker.stop()\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[codecarbon INFO @ 12:02:31] [setup] RAM Tracking...\n[codecarbon INFO @ 12:02:31] [setup] CPU Tracking...\n[codecarbon WARNING @ 12:02:31] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon INFO @ 12:02:32] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n[codecarbon INFO @ 12:02:32] [setup] GPU Tracking...\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n[codecarbon INFO @ 12:02:32] No GPU found.\n[codecarbon INFO @ 12:02:32] >>> Tracker's metadata:\n[codecarbon INFO @ 12:02:32]   Platform system: Linux-5.15.0-1073-azure-x86_64-with-glibc2.31\n[codecarbon INFO @ 12:02:32]   Python version: 3.10.14\n[codecarbon INFO @ 12:02:32]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 12:02:32]   Available RAM : 31.343 GB\n[codecarbon INFO @ 12:02:32]   CPU count: 4\n[codecarbon INFO @ 12:02:32]   CPU model: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n[codecarbon INFO @ 12:02:32]   GPU count: None\n[codecarbon INFO @ 12:02:32]   GPU model: None\n[codecarbon WARNING @ 12:02:33] Cloud provider 'azure' do not publish electricity carbon intensity. Using country value instead.\n[codecarbon INFO @ 12:02:33] Saving emissions data to file /mnt/batch/tasks/shared/LS_root/mounts/clusters/works-sost-cpu/code/Users/paul.vanb/emissions.csv\n[codecarbon INFO @ 12:02:48] Energy consumed for RAM : 0.000013 kWh. RAM Power : 3.091140747070313 W\n[codecarbon INFO @ 12:02:48] Energy consumed for all CPUs : 0.000407 kWh. Total CPU Power : 97.5 W\n[codecarbon INFO @ 12:02:48] 0.000419 kWh of electricity used since the beginning.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "CPU times: user 34.3 s, sys: 51.9 ms, total: 34.3 s\nWall time: 20.1 s\n"
        }
      ],
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTK5TMmvtryP",
        "outputId": "3e488b30-0b67-4af7-c84f-7505c4ef1b29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(emmissions)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "4.0572470961205693e-05\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdUr_pZZ4oEa",
        "outputId": "09e589ee-4f82-435b-f346-0cb9d1fa060b",
        "gather": {
          "logged": 1739966572791
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando with"
      ],
      "metadata": {
        "id": "TH9Dp1VU5OoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with EmissionsTracker() as tracker:\n",
        "  generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres un asistente majo\",\n",
        "         )\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[codecarbon INFO @ 12:02:51] [setup] RAM Tracking...\n[codecarbon INFO @ 12:02:51] [setup] CPU Tracking...\n[codecarbon WARNING @ 12:02:51] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon INFO @ 12:02:52] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n[codecarbon INFO @ 12:02:52] [setup] GPU Tracking...\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n[codecarbon INFO @ 12:02:52] No GPU found.\n[codecarbon INFO @ 12:02:52] >>> Tracker's metadata:\n[codecarbon INFO @ 12:02:52]   Platform system: Linux-5.15.0-1073-azure-x86_64-with-glibc2.31\n[codecarbon INFO @ 12:02:52]   Python version: 3.10.14\n[codecarbon INFO @ 12:02:52]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 12:02:52]   Available RAM : 31.343 GB\n[codecarbon INFO @ 12:02:52]   CPU count: 4\n[codecarbon INFO @ 12:02:52]   CPU model: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n[codecarbon INFO @ 12:02:52]   GPU count: None\n[codecarbon INFO @ 12:02:52]   GPU model: None\n[codecarbon WARNING @ 12:02:52] Cloud provider 'azure' do not publish electricity carbon intensity. Using country value instead.\n[codecarbon INFO @ 12:02:53] Saving emissions data to file /mnt/batch/tasks/shared/LS_root/mounts/clusters/works-sost-cpu/code/Users/paul.vanb/emissions.csv\n[codecarbon INFO @ 12:03:08] Energy consumed for RAM : 0.000049 kWh. RAM Power : 11.753727436065674 W\n[codecarbon INFO @ 12:03:08] Energy consumed for all CPUs : 0.000406 kWh. Total CPU Power : 97.5 W\n[codecarbon INFO @ 12:03:08] 0.000455 kWh of electricity used since the beginning.\n[codecarbon INFO @ 12:03:10] Energy consumed for RAM : 0.000056 kWh. RAM Power : 11.753727436065674 W\n[codecarbon INFO @ 12:03:10] Energy consumed for all CPUs : 0.000467 kWh. Total CPU Power : 97.5 W\n[codecarbon INFO @ 12:03:10] 0.000524 kWh of electricity used since the beginning.\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bVginYm4hvK",
        "outputId": "1719c56a-540f-4498-b166-7df01b4a443f",
        "gather": {
          "logged": 1739966591814
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como decorador"
      ],
      "metadata": {
        "id": "LYZbUxTh5RPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@track_emissions(project_name=\"workshop-sostenibilidad\")\n",
        "def answer_to_different_messages(messages):\n",
        "  outputs = []\n",
        "  system_prompt = \"You are a friendly and helpful assistant\"\n",
        "  for message in messages:\n",
        "    output = generate(message)\n",
        "    outputs.append(message)\n",
        "  return outputs\n"
      ],
      "outputs": [],
      "execution_count": 12,
      "metadata": {
        "id": "KfLriy2N1XrQ",
        "gather": {
          "logged": 1739966628292
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "answer_to_different_messages([\"hola!\",\"cómo puedo hacerlo?\",\" gracias!\"])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[codecarbon INFO @ 12:03:49] [setup] RAM Tracking...\n[codecarbon INFO @ 12:03:49] [setup] CPU Tracking...\n[codecarbon WARNING @ 12:03:49] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon INFO @ 12:03:50] CPU Model on constant consumption mode: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n[codecarbon INFO @ 12:03:50] [setup] GPU Tracking...\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n[codecarbon INFO @ 12:03:50] No GPU found.\n[codecarbon INFO @ 12:03:50] >>> Tracker's metadata:\n[codecarbon INFO @ 12:03:50]   Platform system: Linux-5.15.0-1073-azure-x86_64-with-glibc2.31\n[codecarbon INFO @ 12:03:50]   Python version: 3.10.14\n[codecarbon INFO @ 12:03:50]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 12:03:50]   Available RAM : 31.343 GB\n[codecarbon INFO @ 12:03:50]   CPU count: 4\n[codecarbon INFO @ 12:03:50]   CPU model: Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz\n[codecarbon INFO @ 12:03:50]   GPU count: None\n[codecarbon INFO @ 12:03:50]   GPU model: None\n[codecarbon WARNING @ 12:03:50] Cloud provider 'azure' do not publish electricity carbon intensity. Using country value instead.\n[codecarbon INFO @ 12:03:51] Saving emissions data to file /mnt/batch/tasks/shared/LS_root/mounts/clusters/works-sost-cpu/code/Users/paul.vanb/emissions.csv\n[codecarbon INFO @ 12:04:06] Energy consumed for RAM : 0.000049 kWh. RAM Power : 11.753727436065674 W\n[codecarbon INFO @ 12:04:06] Energy consumed for all CPUs : 0.000406 kWh. Total CPU Power : 97.5 W\n[codecarbon INFO @ 12:04:06] 0.000455 kWh of electricity used since the beginning.\n[codecarbon INFO @ 12:04:21] Energy consumed for RAM : 0.000098 kWh. RAM Power : 11.753727436065674 W\n[codecarbon INFO @ 12:04:21] Energy consumed for all CPUs : 0.000813 kWh. Total CPU Power : 97.5 W\n[codecarbon INFO @ 12:04:21] 0.000910 kWh of electricity used since the beginning.\n[codecarbon INFO @ 12:04:36] Energy consumed for RAM : 0.000147 kWh. RAM Power : 11.753727436065674 W\n[codecarbon INFO @ 12:04:36] Energy consumed for all CPUs : 0.001219 kWh. Total CPU Power : 97.5 W\n[codecarbon INFO @ 12:04:36] 0.001366 kWh of electricity used since the beginning.\n[codecarbon INFO @ 12:04:51] Energy consumed for RAM : 0.000196 kWh. RAM Power : 11.753727436065674 W\n[codecarbon INFO @ 12:04:51] Energy consumed for all CPUs : 0.001625 kWh. Total CPU Power : 97.5 W\n[codecarbon INFO @ 12:04:51] 0.001821 kWh of electricity used since the beginning.\n[codecarbon INFO @ 12:04:54] \nGraceful stopping: collecting and writing information.\nPlease wait a few seconds...\n[codecarbon INFO @ 12:04:54] Energy consumed for RAM : 0.000206 kWh. RAM Power : 11.753727436065674 W\n[codecarbon INFO @ 12:04:54] Energy consumed for all CPUs : 0.001709 kWh. Total CPU Power : 97.5 W\n[codecarbon INFO @ 12:04:54] 0.001915 kWh of electricity used since the beginning.\n[codecarbon INFO @ 12:04:55] Done!\n\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 13,
          "data": {
            "text/plain": "['hola!', 'cómo puedo hacerlo?', ' gracias!']"
          },
          "metadata": {}
        }
      ],
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSlx0bA13eda",
        "outputId": "a62ba4e2-9162-4a79-d1de-e12d86328bfc",
        "gather": {
          "logged": 1739966695605
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://mlco2.github.io/codecarbon/output.html#output"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%sh\n",
        "ls"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "SpainAI_Microsoft_Workshop_codecarbon.ipynb\nemissions.csv\nspainai_microsoft_workshop_codecarbon.ipynb.amltmp\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1739966878051
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_emissions = pd.read_csv(\"emissions.csv\")"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "             timestamp             project_name  \\\n0  2025-02-19T12:02:50               codecarbon   \n1  2025-02-19T12:03:10               codecarbon   \n2  2025-02-19T12:04:54  workshop-sostenibilidad   \n\n                                 run_id                         experiment_id  \\\n0  3bfd136c-d8e5-4f4e-a98e-a51239239023  5b0fa12a-3dd7-45bb-9766-cc326314d9f1   \n1  9bc68c95-44f7-4838-9b5e-00de148b7f57  5b0fa12a-3dd7-45bb-9766-cc326314d9f1   \n2  ef6ae2a3-bd22-469f-aa2e-e12b329f5886  5b0fa12a-3dd7-45bb-9766-cc326314d9f1   \n\n    duration  emissions  emissions_rate  cpu_power  gpu_power  ram_power  ...  \\\n0  17.135990   0.000041        0.000002       97.5        0.0   3.091317  ...   \n1  17.254767   0.000044        0.000003       97.5        0.0  11.753727  ...   \n2  63.112533   0.000162        0.000003       97.5        0.0  11.753727  ...   \n\n   cpu_count                                       cpu_model  gpu_count  \\\n0          4  Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz        NaN   \n1          4  Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz        NaN   \n2          4  Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz        NaN   \n\n   gpu_model longitude latitude ram_total_size  tracking_mode  on_cloud  pue  \n0        NaN -122.3303  47.6109      31.343273        process         N  1.0  \n1        NaN -122.3303  47.6109      31.343273        machine         N  1.0  \n2        NaN -122.3303  47.6109      31.343273        machine         N  1.0  \n\n[3 rows x 32 columns]",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>timestamp</th>\n      <th>project_name</th>\n      <th>run_id</th>\n      <th>experiment_id</th>\n      <th>duration</th>\n      <th>emissions</th>\n      <th>emissions_rate</th>\n      <th>cpu_power</th>\n      <th>gpu_power</th>\n      <th>ram_power</th>\n      <th>...</th>\n      <th>cpu_count</th>\n      <th>cpu_model</th>\n      <th>gpu_count</th>\n      <th>gpu_model</th>\n      <th>longitude</th>\n      <th>latitude</th>\n      <th>ram_total_size</th>\n      <th>tracking_mode</th>\n      <th>on_cloud</th>\n      <th>pue</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2025-02-19T12:02:50</td>\n      <td>codecarbon</td>\n      <td>3bfd136c-d8e5-4f4e-a98e-a51239239023</td>\n      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n      <td>17.135990</td>\n      <td>0.000041</td>\n      <td>0.000002</td>\n      <td>97.5</td>\n      <td>0.0</td>\n      <td>3.091317</td>\n      <td>...</td>\n      <td>4</td>\n      <td>Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-122.3303</td>\n      <td>47.6109</td>\n      <td>31.343273</td>\n      <td>process</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2025-02-19T12:03:10</td>\n      <td>codecarbon</td>\n      <td>9bc68c95-44f7-4838-9b5e-00de148b7f57</td>\n      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n      <td>17.254767</td>\n      <td>0.000044</td>\n      <td>0.000003</td>\n      <td>97.5</td>\n      <td>0.0</td>\n      <td>11.753727</td>\n      <td>...</td>\n      <td>4</td>\n      <td>Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-122.3303</td>\n      <td>47.6109</td>\n      <td>31.343273</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2025-02-19T12:04:54</td>\n      <td>workshop-sostenibilidad</td>\n      <td>ef6ae2a3-bd22-469f-aa2e-e12b329f5886</td>\n      <td>5b0fa12a-3dd7-45bb-9766-cc326314d9f1</td>\n      <td>63.112533</td>\n      <td>0.000162</td>\n      <td>0.000003</td>\n      <td>97.5</td>\n      <td>0.0</td>\n      <td>11.753727</td>\n      <td>...</td>\n      <td>4</td>\n      <td>Intel(R) Xeon(R) Platinum 8272CL CPU @ 2.60GHz</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>-122.3303</td>\n      <td>47.6109</td>\n      <td>31.343273</td>\n      <td>machine</td>\n      <td>N</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>3 rows × 32 columns</p>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1739966892343
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de la API\n",
        "\n",
        "https://dashboard.codecarbon.io/"
      ],
      "metadata": {
        "id": "XU5UVLwy4yGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# en local\n",
        "!codecarbon login"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[1m                                                                                                    \u001b[0m\n\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mcodecarbon login [OPTIONS]\u001b[0m\u001b[1m                                                                 \u001b[0m\u001b[1m \u001b[0m\n\u001b[1m                                                                                                    \u001b[0m\n\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m          Show this message and exit.                                                      \u001b[2m│\u001b[0m\n\u001b[2m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvR0gxrO9Dta",
        "outputId": "c766c678-0056-490e-ed4e-b7bb00641f64"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!codecarbon login --help"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[1m                                                                                \u001b[0m\r\n\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mcodecarbon login [OPTIONS]\u001b[0m\u001b[1m                                             \u001b[0m\u001b[1m \u001b[0m\r\n\u001b[1m                                                                                \u001b[0m\r\n\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\r\n\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m          Show this message and exit.                                  \u001b[2m│\u001b[0m\r\n\u001b[2m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\r\n\r\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IafItGRI81qx",
        "outputId": "7ac34f81-98eb-4f4d-85fe-587e1fb89411"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!codecarbon login"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "⠸\u001b[0m Please complete authentication in your browser.^C\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKi-Cmup550e",
        "outputId": "8267089a-1750-4752-f957-8c48d646f978"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!codecarbon init"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\u001b[33mUsage: \u001b[0mcodecarbon [OPTIONS] COMMAND [ARGS]...\n\u001b[2mTry \u001b[0m\u001b[2;34m'codecarbon \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m─────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m No such command 'init'.                                                                          \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S70rlEtR4y_i",
        "outputId": "77f447b5-d191-4ff4-bfdb-bcfe5436e38e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!codecarbon monitor\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ciE7N8yd5tpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "# Start tracking\n",
        "tracker = EmissionsTracker(tracking_mode=\"process\", save_to_api=True, )\n",
        "tracker.start()\n",
        "\n",
        "generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres un asistente majo\",\n",
        "         )\n",
        "emmissions = tracker.stop()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[codecarbon INFO @ 21:23:56] [setup] RAM Tracking...\n[codecarbon INFO @ 21:23:56] [setup] CPU Tracking...\n[codecarbon WARNING @ 21:23:56] No CPU tracking mode found. Falling back on CPU constant mode. \n Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n\n[codecarbon WARNING @ 21:23:57] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n[codecarbon INFO @ 21:23:57] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 21:23:57] [setup] GPU Tracking...\n[codecarbon INFO @ 21:23:57] Tracking Nvidia GPU via pynvml\n[codecarbon INFO @ 21:23:57] >>> Tracker's metadata:\n[codecarbon INFO @ 21:23:57]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n[codecarbon INFO @ 21:23:57]   Python version: 3.11.11\n[codecarbon INFO @ 21:23:57]   CodeCarbon version: 2.8.3\n[codecarbon INFO @ 21:23:57]   Available RAM : 12.675 GB\n[codecarbon INFO @ 21:23:57]   CPU count: 2\n[codecarbon INFO @ 21:23:57]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n[codecarbon INFO @ 21:23:57]   GPU count: 1\n[codecarbon INFO @ 21:23:57]   GPU model: 1 x Tesla T4\n[codecarbon INFO @ 21:23:58] Saving emissions data to file /content/emissions.csv\n[codecarbon ERROR @ 21:23:58] ApiClient Error when calling the API on https://api.codecarbon.io/runs with : {\"timestamp\": \"2025-02-18T21:23:58.201687+00:00\", \"experiment_id\": \"5b0fa12a-3dd7-45bb-9766-cc326314d9f1\", \"os\": \"Linux-6.1.85+-x86_64-with-glibc2.35\", \"python_version\": \"3.11.11\", \"codecarbon_version\": \"2.8.3\", \"cpu_count\": 2, \"cpu_model\": \"Intel(R) Xeon(R) CPU @ 2.00GHz\", \"gpu_count\": 1, \"gpu_model\": \"1 x Tesla T4\", \"longitude\": 6.6, \"latitude\": 53.2, \"region\": null, \"provider\": null, \"ram_total_size\": 12.67477035522461, \"tracking_mode\": \"process\"}\n[codecarbon ERROR @ 21:23:58] ApiClient API return http code 403 and answer : {\"detail\":\"Not allowed to perform this action\"}\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'str'>\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "[codecarbon INFO @ 21:24:03] Energy consumed for RAM : 0.000001 kWh. RAM Power : 0.6319141387939453 W\n[codecarbon INFO @ 21:24:03] Energy consumed for all CPUs : 0.000066 kWh. Total CPU Power : 42.5 W\n[codecarbon INFO @ 21:24:03] Energy consumed for all GPUs : 0.000063 kWh. Total GPU Power : 40.90357215002361 W\n[codecarbon INFO @ 21:24:03] 0.000130 kWh of electricity used since the beginning.\n[codecarbon ERROR @ 21:24:03] ApiClient Error when calling the API on https://api.codecarbon.io/runs with : {\"timestamp\": \"2025-02-18T21:24:03.875871+00:00\", \"experiment_id\": \"5b0fa12a-3dd7-45bb-9766-cc326314d9f1\", \"os\": \"Linux-6.1.85+-x86_64-with-glibc2.35\", \"python_version\": \"3.11.11\", \"codecarbon_version\": \"2.8.3\", \"cpu_count\": 2, \"cpu_model\": \"Intel(R) Xeon(R) CPU @ 2.00GHz\", \"gpu_count\": 1, \"gpu_model\": \"1 x Tesla T4\", \"longitude\": 6.6, \"latitude\": 53.2, \"region\": null, \"provider\": null, \"ram_total_size\": 12.67477035522461, \"tracking_mode\": \"process\"}\n[codecarbon ERROR @ 21:24:03] ApiClient API return http code 403 and answer : {\"detail\":\"Not allowed to perform this action\"}\n[codecarbon ERROR @ 21:24:03] ApiClient.add_emission still no run_id, aborting for this time !\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'str'>\nCPU times: user 3.34 s, sys: 30.7 ms, total: 3.38 s\nWall time: 7.25 s\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xG0X4Rg8hym",
        "outputId": "aa154879-5fdf-4db8-e11d-8c8b96c90859"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "ROD1radl8jfp"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fcef54490bae4d83b6ef6d36bbf46d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "HTMLModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HTMLView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_b715b9944fa547a4b65e554dc5d9fd1e",
            "value": "Loading checkpoint shards: 100%",
            "style": "IPY_MODEL_11dec34858fe493ba356c42e98dec0c7",
            "placeholder": "​",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "7240007e78444219878b593e7b49a93a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "11dec34858fe493ba356c42e98dec0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e980c20fece4573a9a522f1e5d84e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HBoxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_51921656a92c437e82dc96a0d14246e3",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcef54490bae4d83b6ef6d36bbf46d84",
              "IPY_MODEL_1ed491b9ba624120bc6b3e6db5b98cf6",
              "IPY_MODEL_102b824d816e43259751e828e46a2566"
            ]
          }
        },
        "51921656a92c437e82dc96a0d14246e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "102b824d816e43259751e828e46a2566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "HTMLModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HTMLView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_2523f8c0d6274cdd977884b215f516a3",
            "value": " 2/2 [00:37&lt;00:00, 17.82s/it]",
            "style": "IPY_MODEL_fb068428db9148efbf737dbff9e01867",
            "placeholder": "​",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "b715b9944fa547a4b65e554dc5d9fd1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "fb068428db9148efbf737dbff9e01867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd317caff4c8479fa02a4403361e2463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "ProgressStyleModel",
            "_model_module": "@jupyter-widgets/controls",
            "description_width": "",
            "_view_name": "StyleView",
            "_view_module": "@jupyter-widgets/base",
            "_view_count": null,
            "bar_color": null,
            "_model_module_version": "1.5.0"
          }
        },
        "1ed491b9ba624120bc6b3e6db5b98cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "FloatProgressModel",
            "_model_module": "@jupyter-widgets/controls",
            "max": 2,
            "bar_style": "success",
            "_view_name": "ProgressView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_7240007e78444219878b593e7b49a93a",
            "orientation": "horizontal",
            "value": 2,
            "style": "IPY_MODEL_bd317caff4c8479fa02a4403361e2463",
            "min": 0,
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "2523f8c0d6274cdd977884b215f516a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        }
      }
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}