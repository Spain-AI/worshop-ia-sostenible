{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0e980c20fece4573a9a522f1e5d84e00": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fcef54490bae4d83b6ef6d36bbf46d84",
              "IPY_MODEL_1ed491b9ba624120bc6b3e6db5b98cf6",
              "IPY_MODEL_102b824d816e43259751e828e46a2566"
            ],
            "layout": "IPY_MODEL_51921656a92c437e82dc96a0d14246e3"
          }
        },
        "fcef54490bae4d83b6ef6d36bbf46d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b715b9944fa547a4b65e554dc5d9fd1e",
            "placeholder": "​",
            "style": "IPY_MODEL_11dec34858fe493ba356c42e98dec0c7",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "1ed491b9ba624120bc6b3e6db5b98cf6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7240007e78444219878b593e7b49a93a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bd317caff4c8479fa02a4403361e2463",
            "value": 2
          }
        },
        "102b824d816e43259751e828e46a2566": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2523f8c0d6274cdd977884b215f516a3",
            "placeholder": "​",
            "style": "IPY_MODEL_fb068428db9148efbf737dbff9e01867",
            "value": " 2/2 [00:37&lt;00:00, 17.82s/it]"
          }
        },
        "51921656a92c437e82dc96a0d14246e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b715b9944fa547a4b65e554dc5d9fd1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "11dec34858fe493ba356c42e98dec0c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7240007e78444219878b593e7b49a93a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd317caff4c8479fa02a4403361e2463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2523f8c0d6274cdd977884b215f516a3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb068428db9148efbf737dbff9e01867": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tracking del consumo energético con codecarbon"
      ],
      "metadata": {
        "id": "-2OJtDt541b3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gfj8i33vs25s",
        "outputId": "95c62742-1379-4e25-fd3f-a5cf08809fef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: codecarbon in /usr/local/lib/python3.11/dist-packages (2.8.3)\n",
            "Requirement already satisfied: arrow in /usr/local/lib/python3.11/dist-packages (from codecarbon) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from codecarbon) (8.1.8)\n",
            "Requirement already satisfied: fief-client[cli] in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.20.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.2.2)\n",
            "Requirement already satisfied: prometheus-client in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.21.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from codecarbon) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from codecarbon) (9.0.0)\n",
            "Requirement already satisfied: pynvml in /usr/local/lib/python3.11/dist-packages (from codecarbon) (12.0.0)\n",
            "Requirement already satisfied: questionary in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.1.0)\n",
            "Requirement already satisfied: rapidfuzz in /usr/local/lib/python3.11/dist-packages (from codecarbon) (3.12.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from codecarbon) (2.32.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from codecarbon) (13.9.4)\n",
            "Requirement already satisfied: typer in /usr/local/lib/python3.11/dist-packages (from codecarbon) (0.15.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.0 in /usr/local/lib/python3.11/dist-packages (from arrow->codecarbon) (2.8.2)\n",
            "Requirement already satisfied: types-python-dateutil>=2.8.10 in /usr/local/lib/python3.11/dist-packages (from arrow->codecarbon) (2.9.0.20241206)\n",
            "Requirement already satisfied: httpx<0.28.0,>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from fief-client[cli]->codecarbon) (0.27.2)\n",
            "Requirement already satisfied: jwcrypto<2.0.0,>=1.4 in /usr/local/lib/python3.11/dist-packages (from fief-client[cli]->codecarbon) (1.5.6)\n",
            "Requirement already satisfied: yaspin in /usr/local/lib/python3.11/dist-packages (from fief-client[cli]->codecarbon) (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (1.26.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->codecarbon) (2025.1)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0a0,>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pynvml->codecarbon) (12.570.86)\n",
            "Requirement already satisfied: prompt_toolkit<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from questionary->codecarbon) (3.0.50)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->codecarbon) (2025.1.31)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->codecarbon) (2.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (4.12.2)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer->codecarbon) (1.5.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.0.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.21.3->fief-client[cli]->codecarbon) (0.14.0)\n",
            "Requirement already satisfied: cryptography>=3.4 in /usr/local/lib/python3.11/dist-packages (from jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (43.0.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->codecarbon) (0.1.2)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt_toolkit<4.0,>=2.0->questionary->codecarbon) (0.2.13)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7.0->arrow->codecarbon) (1.17.0)\n",
            "Requirement already satisfied: termcolor<2.4.0,>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from yaspin->fief-client[cli]->codecarbon) (2.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=3.4->jwcrypto<2.0.0,>=1.4->fief-client[cli]->codecarbon) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip -q install transformers==4.48.3 accelerate\n",
        "!pip install -q sentencepiece tokenizer\n",
        "!pip install codecarbon"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "\n",
        "import textwrap\n",
        "\n",
        "from codecarbon import EmissionsTracker\n",
        "\n",
        "import torch"
      ],
      "metadata": {
        "id": "0LyAUyQ4s3_7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Modelo\n",
        "Usaremos mocrosoft Phi"
      ],
      "metadata": {
        "id": "Zv2Oll0647jO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\",\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=\"auto\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "0e980c20fece4573a9a522f1e5d84e00",
            "fcef54490bae4d83b6ef6d36bbf46d84",
            "1ed491b9ba624120bc6b3e6db5b98cf6",
            "102b824d816e43259751e828e46a2566",
            "51921656a92c437e82dc96a0d14246e3",
            "b715b9944fa547a4b65e554dc5d9fd1e",
            "11dec34858fe493ba356c42e98dec0c7",
            "7240007e78444219878b593e7b49a93a",
            "bd317caff4c8479fa02a4403361e2463",
            "2523f8c0d6274cdd977884b215f516a3",
            "fb068428db9148efbf737dbff9e01867"
          ]
        },
        "id": "4RQLYiQHtdJV",
        "outputId": "064a0a14-77ff-4b88-ab6a-45dd23c2da67"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0e980c20fece4573a9a522f1e5d84e00"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prueba con el texto de ejemplo de huggingface"
      ],
      "metadata": {
        "id": "AWbfhHfD5Apa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful AI assistant.\"},\n",
        "    {\"role\": \"user\", \"content\": \"Can you provide ways to eat combinations of bananas and dragonfruits?\"},\n",
        "    {\"role\": \"assistant\", \"content\": \"Sure! Here are some ways to eat bananas and dragonfruits together: 1. Banana and dragonfruit smoothie: Blend bananas and dragonfruits together with some milk and honey. 2. Banana and dragonfruit salad: Mix sliced bananas and dragonfruits together with some lemon juice and honey.\"},\n",
        "    {\"role\": \"user\", \"content\": \"What about solving an 2x + 3 = 7 equation?\"},\n",
        "]\n",
        "\n",
        "pipe = pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "generation_args = {\n",
        "    \"max_new_tokens\": 500,\n",
        "    \"return_full_text\": False,\n",
        "    \"temperature\": 0.0,\n",
        "    \"do_sample\": False,\n",
        "}\n",
        "\n",
        "output = pipe(messages, **generation_args)\n",
        "print(output[0]['generated_text'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_A4S7myy4JL",
        "outputId": "dd57c30c-143e-43ce-c411-ba2151d5e273"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
            "`get_max_cache()` is deprecated for all Cache classes. Use `get_max_cache_shape()` instead. Calling `get_max_cache()` will raise error from v4.48\n",
            "WARNING:transformers_modules.microsoft.Phi-3.5-mini-instruct.af0dfb8029e8a74545d0736d30cb6b58d2f0f3f0.modeling_phi3:You are not running the flash-attention implementation, expect numerical differences.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " To solve the equation 2x + 3 = 7, follow these steps:\n",
            "\n",
            "Step 1: Isolate the term with the variable (2x) by subtracting 3 from both sides of the equation.\n",
            "2x + 3 - 3 = 7 - 3\n",
            "2x = 4\n",
            "\n",
            "Step 2: Solve for x by dividing both sides of the equation by the coefficient of x, which is 2.\n",
            "2x / 2 = 4 / 2\n",
            "x = 2\n",
            "\n",
            "So, the solution to the equation 2x + 3 = 7 is x = 2.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creamos funciones para organizar output"
      ],
      "metadata": {
        "id": "PlQRIo-N3B_J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def wrap_text(text, width=100):\n",
        "    lines = text.split('\\n')\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "    return wrapped_text\n",
        "\n",
        "def generate(input_text, system_prompt=\"\", max_length=1024):\n",
        "    if system_prompt != \"\":\n",
        "        system_prompt = system_prompt\n",
        "    else:\n",
        "        system_prompt = \"You are a friendly and helpful assistant\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\":  input_text},\n",
        "    ]\n",
        "\n",
        "    output = pipe(messages, **generation_args)\n",
        "    text = output[0]['generated_text']\n",
        "\n",
        "    wrapped_text = wrap_text(text)\n",
        "    return wrapped_text"
      ],
      "metadata": {
        "id": "5r8b3DKDtpai"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres marcoaurelio, hijo de thor\",\n",
        "         )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "lnP0vvHsybz4",
        "outputId": "3eea63eb-8a9f-45d0-ced3-57f7fe918b28"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Soy un modelo de lenguaje de IA desarrollado por Microsoft. No soy Marco Aurélio, un renombrado\\nabogado brasileño, sino una inteligencia artificial diseñada para ayudar con una amplia gama de\\npreguntas y tareas proporcionando información y asistencia.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ejemplo de codecarbon"
      ],
      "metadata": {
        "id": "FH_iNESv5LjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# Start tracking\n",
        "tracker = EmissionsTracker(tracking_mode=\"process\") # save_to_api=True,\n",
        "tracker.start()\n",
        "\n",
        "generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres un asistente majo\",\n",
        "         )\n",
        "emmissions = tracker.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cTK5TMmvtryP",
        "outputId": "3e488b30-0b67-4af7-c84f-7505c4ef1b29"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 21:22:11] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 21:22:11] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 21:22:11] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 21:22:12] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 21:22:12] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 21:22:12] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 21:22:12] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 21:22:12] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 21:22:12]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 21:22:12]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 21:22:12]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 21:22:12]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 21:22:12]   CPU count: 2\n",
            "[codecarbon INFO @ 21:22:12]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 21:22:12]   GPU count: 1\n",
            "[codecarbon INFO @ 21:22:13]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 21:22:13] Saving emissions data to file /content/emissions.csv\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 21:22:15] Energy consumed for RAM : 0.000000 kWh. RAM Power : 0.6292233467102051 W\n",
            "[codecarbon INFO @ 21:22:15] Energy consumed for all CPUs : 0.000030 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 21:22:15] Energy consumed for all GPUs : 0.000049 kWh. Total GPU Power : 68.34647735470341 W\n",
            "[codecarbon INFO @ 21:22:15] 0.000080 kWh of electricity used since the beginning.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.63 s, sys: 28.6 ms, total: 2.66 s\n",
            "Wall time: 3.96 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(emmissions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdUr_pZZ4oEa",
        "outputId": "09e589ee-4f82-435b-f346-0cb9d1fa060b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.1363749412053022e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usando with"
      ],
      "metadata": {
        "id": "TH9Dp1VU5OoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with EmissionsTracker() as tracker:\n",
        "  generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres un asistente majo\",\n",
        "         )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bVginYm4hvK",
        "outputId": "1719c56a-540f-4498-b166-7df01b4a443f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 21:22:15] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 21:22:15] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 21:22:15] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 21:22:16] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 21:22:16] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 21:22:16] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 21:22:16] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 21:22:16] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 21:22:16]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 21:22:16]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 21:22:16]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 21:22:16]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 21:22:16]   CPU count: 2\n",
            "[codecarbon INFO @ 21:22:16]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 21:22:16]   GPU count: 1\n",
            "[codecarbon INFO @ 21:22:16]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 21:22:17] Saving emissions data to file /content/emissions.csv\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 21:22:19] Energy consumed for RAM : 0.000004 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 21:22:19] Energy consumed for all CPUs : 0.000033 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 21:22:19] Energy consumed for all GPUs : 0.000052 kWh. Total GPU Power : 66.39812407227797 W\n",
            "[codecarbon INFO @ 21:22:19] 0.000089 kWh of electricity used since the beginning.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como decorador"
      ],
      "metadata": {
        "id": "LYZbUxTh5RPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from codecarbon import track_emissions\n",
        "@track_emissions(project_name=\"workshop-sostenibilidad\")\n",
        "def answer_to_different_messages(messages):\n",
        "  outputs = []\n",
        "  system_prompt = \"You are a friendly and helpful assistant\"\n",
        "  for message in messages:\n",
        "    output = generate(message)\n",
        "    outputs.append(message)\n",
        "  return outputs\n"
      ],
      "metadata": {
        "id": "KfLriy2N1XrQ"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer_to_different_messages([\"hola!\",\"cómo puedo hacerlo?\",\" gracias!\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSlx0bA13eda",
        "outputId": "a62ba4e2-9162-4a79-d1de-e12d86328bfc"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 21:22:19] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 21:22:19] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 21:22:19] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 21:22:21] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 21:22:21] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 21:22:21] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 21:22:21] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 21:22:21] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 21:22:21]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 21:22:21]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 21:22:21]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 21:22:21]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 21:22:21]   CPU count: 2\n",
            "[codecarbon INFO @ 21:22:21]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 21:22:21]   GPU count: 1\n",
            "[codecarbon INFO @ 21:22:21]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 21:22:21] Saving emissions data to file /content/emissions.csv\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n",
            "[codecarbon INFO @ 21:22:31] \n",
            "Graceful stopping: collecting and writing information.\n",
            "Please wait a few seconds...\n",
            "[codecarbon INFO @ 21:22:31] Energy consumed for RAM : 0.000013 kWh. RAM Power : 4.7530388832092285 W\n",
            "[codecarbon INFO @ 21:22:31] Energy consumed for all CPUs : 0.000119 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 21:22:31] Energy consumed for all GPUs : 0.000192 kWh. Total GPU Power : 68.40541530983076 W\n",
            "[codecarbon INFO @ 21:22:31] 0.000324 kWh of electricity used since the beginning.\n",
            "[codecarbon INFO @ 21:22:31] Done!\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hola!', 'cómo puedo hacerlo?', ' gracias!']"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Uso de la API\n",
        "\n",
        "https://dashboard.codecarbon.io/"
      ],
      "metadata": {
        "id": "XU5UVLwy4yGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# en local\n",
        "#!codecarbon login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvR0gxrO9Dta",
        "outputId": "c766c678-0056-490e-ed4e-b7bb00641f64"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m                                                                                                    \u001b[0m\n",
            "\u001b[1m \u001b[0m\u001b[1;33mUsage: \u001b[0m\u001b[1mcodecarbon login [OPTIONS]\u001b[0m\u001b[1m                                                                 \u001b[0m\u001b[1m \u001b[0m\n",
            "\u001b[1m                                                                                                    \u001b[0m\n",
            "\u001b[2m╭─\u001b[0m\u001b[2m Options \u001b[0m\u001b[2m───────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[2m─╮\u001b[0m\n",
            "\u001b[2m│\u001b[0m \u001b[1;36m-\u001b[0m\u001b[1;36m-help\u001b[0m          Show this message and exit.                                                      \u001b[2m│\u001b[0m\n",
            "\u001b[2m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!codecarbon config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IafItGRI81qx",
        "outputId": "7ac34f81-98eb-4f4d-85fe-587e1fb89411"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Welcome to CodeCarbon configuration wizard\n",
            "Existing global config file found :\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/codecarbon/cli/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m177\u001b[0m in \u001b[92mconfig\u001b[0m                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m174 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m global_path.exists():                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mExisting global config file found :\u001b[0m\u001b[33m\"\u001b[0m)                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m177 \u001b[2m│   │   \u001b[0m\u001b[1;4mshow_config(global_path)\u001b[0m                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m178 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m179 \u001b[0m\u001b[2m│   │   \u001b[0muse_config = questionary_prompt(                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m180 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mUse existing global ~/.codecarbon.config to configure or create a new file \u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m──────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m global_path = \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[33m'/root/.codecarbon.config'\u001b[0m\u001b[1m)\u001b[0m \u001b[33m│\u001b[0m                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        home = \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[33m'/root'\u001b[0m\u001b[1m)\u001b[0m                    \u001b[33m│\u001b[0m                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰─────────────────────────────────────────────────────╯\u001b[0m                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/codecarbon/cli/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m67\u001b[0m in \u001b[92mshow_config\u001b[0m                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 64 \u001b[0m\u001b[2m│   \u001b[0md = get_config(path)                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 65 \u001b[0m\u001b[2m│   \u001b[0mapi_endpoint = get_api_endpoint(path)                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 66 \u001b[0m\u001b[2m│   \u001b[0mapi = ApiClient(endpoint_url=api_endpoint)                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 67 \u001b[2m│   \u001b[0mapi.set_access_token(\u001b[1;4m_get_access_token()\u001b[0m)                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 68 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mCurrent configuration : \u001b[0m\u001b[33m\\n\u001b[0m\u001b[33m\"\u001b[0m)                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 69 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mConfig file content : \u001b[0m\u001b[33m\"\u001b[0m)                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 70 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(d)                                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m───────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m───────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m          api = \u001b[1m<\u001b[0m\u001b[1;95mcodecarbon.core.api_client.ApiClient\u001b[0m\u001b[39m object at \u001b[0m\u001b[94m0x7c6434c24b10\u001b[0m\u001b[1m>\u001b[0m \u001b[33m│\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m api_endpoint = \u001b[33m'https://api.codecarbon.io'\u001b[0m                                     \u001b[33m│\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            d = \u001b[1m{\u001b[0m\u001b[33m'api_endpoint'\u001b[0m: \u001b[33m'https://api.codecarbon.io'\u001b[0m\u001b[1m}\u001b[0m                   \u001b[33m│\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         path = \u001b[1;35mPosixPath\u001b[0m\u001b[1m(\u001b[0m\u001b[33m'/root/.codecarbon.config'\u001b[0m\u001b[1m)\u001b[0m                           \u001b[33m│\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────────────────────────────────╯\u001b[0m               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/codecarbon/cli/\u001b[0m\u001b[1;33mmain.py\u001b[0m:\u001b[94m110\u001b[0m in \u001b[92m_get_access_token\u001b[0m          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m107 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m108 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m109 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_get_access_token\u001b[0m():                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m110 \u001b[2m│   \u001b[0maccess_token_info = \u001b[1;4mget_fief_auth().access_token_info()\u001b[0m                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   \u001b[0maccess_token = access_token_info[\u001b[33m\"\u001b[0m\u001b[33maccess_token\u001b[0m\u001b[33m\"\u001b[0m]                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m access_token                                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.11/dist-packages/fief_client/integrations/\u001b[0m\u001b[1;33mcli.py\u001b[0m:\u001b[94m149\u001b[0m in \u001b[92maccess_token_info\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m146 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m:raises: `fief_client.FiefAccessTokenExpired` if the access token is expired and\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m147 \u001b[0m\u001b[2;33m│   │   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m148 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m._tokens \u001b[95mis\u001b[0m \u001b[94mNone\u001b[0m:                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m149 \u001b[2m│   │   │   \u001b[0m\u001b[1;4;94mraise\u001b[0m\u001b[1;4m FiefAuthNotAuthenticatedError()\u001b[0m                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m150 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m151 \u001b[0m\u001b[2m│   │   \u001b[0maccess_token = \u001b[96mself\u001b[0m._tokens[\u001b[33m\"\u001b[0m\u001b[33maccess_token\u001b[0m\u001b[33m\"\u001b[0m]                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m152 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mtry\u001b[0m:                                                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m─────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m─────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m refresh = \u001b[94mTrue\u001b[0m                                                             \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m    self = \u001b[1m<\u001b[0m\u001b[1;95mfief_client.integrations.cli.FiefAuth\u001b[0m\u001b[39m object at \u001b[0m\u001b[94m0x7c6434c28d50\u001b[0m\u001b[1m>\u001b[0m \u001b[33m│\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰────────────────────────────────────────────────────────────────────────────╯\u001b[0m                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mFiefAuthNotAuthenticatedError\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!codecarbon login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lKi-Cmup550e",
        "outputId": "8267089a-1750-4752-f957-8c48d646f978"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⠸\u001b[0m Please complete authentication in your browser.^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!codecarbon init"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S70rlEtR4y_i",
        "outputId": "77f447b5-d191-4ff4-bfdb-bcfe5436e38e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mUsage: \u001b[0mcodecarbon [OPTIONS] COMMAND [ARGS]...\n",
            "\u001b[2mTry \u001b[0m\u001b[2;34m'codecarbon \u001b[0m\u001b[1;2;34m-\u001b[0m\u001b[1;2;34m-help\u001b[0m\u001b[2;34m'\u001b[0m\u001b[2m for help.\u001b[0m\n",
            "\u001b[31m╭─\u001b[0m\u001b[31m Error \u001b[0m\u001b[31m─────────────────────────────────────────────────────────────────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m No such command 'init'.                                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!codecarbon monitor\n"
      ],
      "metadata": {
        "id": "ciE7N8yd5tpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "%%time\n",
        "# Start tracking\n",
        "tracker = EmissionsTracker(tracking_mode=\"process\", save_to_api=True, )\n",
        "tracker.start()\n",
        "\n",
        "generate('¿Quién eres?',\n",
        "         system_prompt=\"Eres un asistente majo\",\n",
        "         )\n",
        "emmissions = tracker.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xG0X4Rg8hym",
        "outputId": "aa154879-5fdf-4db8-e11d-8c8b96c90859"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 21:23:56] [setup] RAM Tracking...\n",
            "[codecarbon INFO @ 21:23:56] [setup] CPU Tracking...\n",
            "[codecarbon WARNING @ 21:23:56] No CPU tracking mode found. Falling back on CPU constant mode. \n",
            " Linux OS detected: Please ensure RAPL files exist at \\sys\\class\\powercap\\intel-rapl to measure CPU\n",
            "\n",
            "[codecarbon WARNING @ 21:23:57] We saw that you have a Intel(R) Xeon(R) CPU @ 2.00GHz but we don't know it. Please contact us.\n",
            "[codecarbon INFO @ 21:23:57] CPU Model on constant consumption mode: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 21:23:57] [setup] GPU Tracking...\n",
            "[codecarbon INFO @ 21:23:57] Tracking Nvidia GPU via pynvml\n",
            "[codecarbon INFO @ 21:23:57] >>> Tracker's metadata:\n",
            "[codecarbon INFO @ 21:23:57]   Platform system: Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "[codecarbon INFO @ 21:23:57]   Python version: 3.11.11\n",
            "[codecarbon INFO @ 21:23:57]   CodeCarbon version: 2.8.3\n",
            "[codecarbon INFO @ 21:23:57]   Available RAM : 12.675 GB\n",
            "[codecarbon INFO @ 21:23:57]   CPU count: 2\n",
            "[codecarbon INFO @ 21:23:57]   CPU model: Intel(R) Xeon(R) CPU @ 2.00GHz\n",
            "[codecarbon INFO @ 21:23:57]   GPU count: 1\n",
            "[codecarbon INFO @ 21:23:57]   GPU model: 1 x Tesla T4\n",
            "[codecarbon INFO @ 21:23:58] Saving emissions data to file /content/emissions.csv\n",
            "[codecarbon ERROR @ 21:23:58] ApiClient Error when calling the API on https://api.codecarbon.io/runs with : {\"timestamp\": \"2025-02-18T21:23:58.201687+00:00\", \"experiment_id\": \"5b0fa12a-3dd7-45bb-9766-cc326314d9f1\", \"os\": \"Linux-6.1.85+-x86_64-with-glibc2.35\", \"python_version\": \"3.11.11\", \"codecarbon_version\": \"2.8.3\", \"cpu_count\": 2, \"cpu_model\": \"Intel(R) Xeon(R) CPU @ 2.00GHz\", \"gpu_count\": 1, \"gpu_model\": \"1 x Tesla T4\", \"longitude\": 6.6, \"latitude\": 53.2, \"region\": null, \"provider\": null, \"ram_total_size\": 12.67477035522461, \"tracking_mode\": \"process\"}\n",
            "[codecarbon ERROR @ 21:23:58] ApiClient API return http code 403 and answer : {\"detail\":\"Not allowed to perform this action\"}\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[codecarbon INFO @ 21:24:03] Energy consumed for RAM : 0.000001 kWh. RAM Power : 0.6319141387939453 W\n",
            "[codecarbon INFO @ 21:24:03] Energy consumed for all CPUs : 0.000066 kWh. Total CPU Power : 42.5 W\n",
            "[codecarbon INFO @ 21:24:03] Energy consumed for all GPUs : 0.000063 kWh. Total GPU Power : 40.90357215002361 W\n",
            "[codecarbon INFO @ 21:24:03] 0.000130 kWh of electricity used since the beginning.\n",
            "[codecarbon ERROR @ 21:24:03] ApiClient Error when calling the API on https://api.codecarbon.io/runs with : {\"timestamp\": \"2025-02-18T21:24:03.875871+00:00\", \"experiment_id\": \"5b0fa12a-3dd7-45bb-9766-cc326314d9f1\", \"os\": \"Linux-6.1.85+-x86_64-with-glibc2.35\", \"python_version\": \"3.11.11\", \"codecarbon_version\": \"2.8.3\", \"cpu_count\": 2, \"cpu_model\": \"Intel(R) Xeon(R) CPU @ 2.00GHz\", \"gpu_count\": 1, \"gpu_model\": \"1 x Tesla T4\", \"longitude\": 6.6, \"latitude\": 53.2, \"region\": null, \"provider\": null, \"ram_total_size\": 12.67477035522461, \"tracking_mode\": \"process\"}\n",
            "[codecarbon ERROR @ 21:24:03] ApiClient API return http code 403 and answer : {\"detail\":\"Not allowed to perform this action\"}\n",
            "[codecarbon ERROR @ 21:24:03] ApiClient.add_emission still no run_id, aborting for this time !\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'str'>\n",
            "CPU times: user 3.34 s, sys: 30.7 ms, total: 3.38 s\n",
            "Wall time: 7.25 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ROD1radl8jfp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}